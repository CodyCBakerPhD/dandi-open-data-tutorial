{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "# Exploring the DANDI Archive\n",
    "\n",
    "This notebook serves as a quick-start guide of the [Distributed Archive for Neurophysiology Data Integration (DANDI)](https://registry.opendata.aws/dandiarchive/).\n",
    "\n",
    "<div style=\"background:#fff2cc; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:-5px 0 5px 20px;\n",
    "color:#000000;\n",
    "width:fit-content; \">\n",
    "    <div style=\"font-weight:bold;\">‚ÑπÔ∏è Definition</div>\n",
    "    A <i>Dandiset</i> is a collection of neurophysiology data and metadata hosted on the <a\n",
    "    href=\"https://dandiarchive.org\" style=\"font-weight:bold;\">DANDI Archive</a>.\n",
    "</div>\n",
    "\n",
    "The DANDI Archive holds hundreds of Dandisets with a diverse range of neurodata modalities.\n",
    "\n",
    "These modalities span the spectrum of microscopy, optogenetics, intracellular and extracellular\n",
    "electrophysiology, and optophysiology.\n",
    "\n",
    "While we cannot hope to completely showcase this diversity here, there are two key examples which provide a good\n",
    "starting point:\n",
    "- [000728 - Visual Coding - Optical Physiology](https://dandiarchive.org/dandiset/000728/) by the Allen Institute for\n",
    "Brain Science (AIBS)\n",
    "- [000409 - Brain Wide Map](https://dandiarchive.org/dandiset/000409/) by the International Brain Laboratory (IBL)\n",
    "\n",
    "For even more [usage guides](https://docs.dandiarchive.org/user-guide-using/exploring-dandisets/),\n",
    "[dandiset-specific tutorials](https://dandi.github.io/example-notebooks/), and general documentation, check out the main\n",
    "[DANDI Docs](https://docs.dandiarchive.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779654-eeee-4708-83cf-245e03303475",
   "metadata": {},
   "source": [
    "### Q: How do I navigate the archive and its datasets?\n",
    "\n",
    "DANDI provides a [web interface](https://dandiarchive.org/dandiset), [REST API](https://api.dandiarchive.org/api/docs/swagger/),\n",
    "and [command-line tool](https://pypi.org/project/dandi/) to help users intuitively navigate the contents.\n",
    "\n",
    "The easiest place to start is the primary [Dandiset listing page](https://dandiarchive.org/dandiset).\n",
    "\n",
    "After scrolling around a while, we choose our first Dandiset from the web interface [000728](https://dandiarchive.org/dandiset/000728/0.240827.1809).\n",
    "\n",
    "We can see the contents by going to the [\"Files\" tab](https://dandiarchive.org/dandiset/000728/0.240827.1809/files).\n",
    "\n",
    "From here, we can see that a Dandiset is organized as a collection of folders organized by subject ID.\n",
    "\n",
    "Each folder contains files named according to session ID or other unique discriminators.\n",
    "\n",
    "```text\n",
    "000728/\n",
    "‚îú‚îÄ‚îÄ sub-691657859/sub-691657859_ses-712919679-StimB_ophys.nwb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sub-691657859_ses-712919679-StimB_ophys.nwb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sub-691657859_ses-710504563-StimA_behavior+image+ophys.nwb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ sub-501800590/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b47b69",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before we start accessing data contents, we will need to install and import some Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q dandi matplotlib remfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import remfile\n",
    "import matplotlib.pyplot as plt\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "from pynwb import read_nwb, NWBHDF5IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ae10",
   "metadata": {},
   "source": [
    "Next, we will initialize our DANDI API client to interact with the archive database and list a few of the\n",
    "available Dandisets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DandiAPIClient()\n",
    "dandisets = list(client.get_dandisets())\n",
    "\n",
    "# Print the dandiset IDs and titles of the first 3 dandisets\n",
    "for dandiset in dandisets[:3]:\n",
    "    print(f\"{dandiset.identifier}: {dandiset.get_raw_metadata()[\"name\"]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000003: Physiological Properties and Behavioral Correlates of Hippocampal Granule Cells and Mossy Cells\n",
      "000004: A NWB-based dataset and processing pipeline of human single-neuron activity during a declarative memory task\n",
      "000005: Electrophysiology data from thalamic and cortical neurons during somatosensation\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9fa4d",
   "metadata": {},
   "source": [
    "Now let's return to our first example Dandiset and list out a few of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dandiset = client.get_dandiset(dandiset_id=\"000728\", version_id=\"0.240827.1809\")\n",
    "assets = list(dandiset.get_assets())\n",
    "\n",
    "# Print the file paths as seen on the DANDI web interface\n",
    "for asset in assets[:3]:\n",
    "    print(asset.get_raw_metadata()[\"path\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-691657859/sub-691657859_ses-712919679-StimB_ophys.nwb\n",
      "sub-501800590/sub-501800590_ses-509522931-StimC_ophys.nwb\n",
      "sub-572569275/sub-572569275_ses-591563201-StimB_ophys.nwb\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47268012",
   "metadata": {},
   "source": [
    "<div style=\"background:#cfe2f3; border:1px solid #000000; border-radius:2px; padding:10px; margin:5px 0 5px 0px;\n",
    "color:#000000; max-width:650px;\">\n",
    "    <div style=\"font-weight:bold;\">üí° Info</div>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "        Notice that we passed a <code style=\"background:#cfe2f3; color:#000000\">version_id</code> in this case.\n",
    "        Dandisets that are published on the archive are given a citable DOI, such as:\n",
    "    </div>\n",
    "    <div style=\"padding-left:20px; margin:5px 0;\">\n",
    "        Allen Institute (2024) <em>Allen Institute - Visual Coding - Optical Physiology</em> (Version 0.240827.1809) [Data set]. DANDI archive. <a href=\"https://doi.org/10.48324/dandi.000728/0.240827.1809\">https://doi.org/10.48324/dandi.000728/0.240827.1809</a>\n",
    "    </div>\n",
    "    <div>These citations should be used in any scientific reuse of the data.</div>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "        Otherwise, the most recent 'draft' state of the Dandiset is used by default and is subject to change by the Dandiset contributors.\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background:#e9e2f8; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:10px 0 5px 0px;\n",
    "color:#000000;\n",
    "width:fit-content; \">\n",
    "    <div style=\"font-weight:bold;\">üß† Learn more</div>\n",
    "    You may have also noticed that in several cases above, we fetched the metadata associated with the Dandisets and\n",
    "    their assets.\n",
    "    <br>\n",
    "    These are very rich models whose full potential is best showcased in the <a href=\"https://docs.dandiarchive\n",
    "    .org/example-notebooks/tutorials/cosyne_2023/advanced_asset_search/#going-beyond\" style=\"font-weight:bold;\">Advanced Search Tutorial</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bcf-ec40-432f-a31f-4477efa205ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q: What kinds of data are hosted and what formats do they use?\n",
    "\n",
    "DANDI accepts a relatively small number of open, community-driven file formats designed according to NIH-accepted\n",
    "data standards*.\n",
    "\n",
    "|                                         <center>Data Standard</center>                                          | <center>Acronym</center>  |                 <center>Domain</center>                  |    <center>Data Format(s)</center>    |\n",
    "|:---------------------------------------------------------------------------------------------------------------:|:-------------------------:|:--------------------------------------------------------:|:-------------------------------------:|\n",
    "|                         <center>[Neurodata Without Borders](https://nwb.org/)</center>                          |   <center>NWB</center>    |     <center>Neurophysiology<br>and behavior</center>     |     <center>HDF5<br>Zarr</center>     |\n",
    "|                 <center>[Brain Imaging Data Structure](https://bids.neuroimaging.io/)</center>                  |   <center>BIDS</center>   |    <center>Neuroimaging<br>(MRI, EEG, etc.)</center>     | <center>NIfTI<br>JSON<br>TSV</center> |\n",
    "|    <center>[Open Microscopy Environment](https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/)</center>    | <center>OME-TIFF</center> |           <center>Microscopy imaging</center>            |         <center>TIFF</center>         |\n",
    "| <center>[Open Microscopy Environment<br>Next Generation File Format](https://ngff.openmicroscopy.org/)</center> | <center>OME-Zarr</center> | <center>Microscopy imaging<br>(cloud-optimized)</center> |         <center>Zarr</center>         |\n",
    "\n",
    "These data standards are specifically designed to integrate multi-modal raw and processed neurodata alongside\n",
    "behavioral data and metadata annotations.\n",
    "\n",
    "The S3 bucket hosting the DANDI archive allows users to take advantage of cloud-native\n",
    "services for scalable data access, computation, visualization, and analysis.\n",
    "\n",
    "This allows DANDI to integrate with many external visualization tools, accessible via the \"Open With\" button on the web interface:\n",
    "- [NWB: Neurosift](https://neurosift.app)\n",
    "    - [Example: 000728/sub-495727000/sub-495727000_ses-51254258-StimC_behavior+image+ophys.nwb](https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/0205b9b1-10c4-467c-b027-20bbbfcce3a0/download/&dandisetId=001172&dandisetVersion=0.260129.0829)\n",
    "- [OME: Neuroglancer](https://github.com/google/neuroglancer)\n",
    "    - [Example: 000026/sub-I58/ses-Hip-CT/micr/](https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22z%22:%5B0.00001513%2C%22m%22%5D%2C%22y%22:%5B0.00001513%2C%22m%22%5D%2C%22x%22:%5B0.00001513%2C%22m%22%5D%7D%2C%22position%22:%5B5257.03564453125%2C4706%2C4218.56396484375%5D%2C%22crossSectionScale%22:21.48356465187443%2C%22projectionScale%22:16384%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22https://dandiarchive.s3.amazonaws.com/zarr/5c37c233-222f-4e60-96e7-a7536e08ef61%22%2C%22tab%22:%22rendering%22%2C%22shaderControls%22:%7B%22normalized%22:%7B%22range%22:%5B23257%2C24764%5D%2C%22window%22:%5B22877%2C25144%5D%7D%7D%2C%22name%22:%22798b8b1b-c88d-42e8-91f8-247fd4282fe7%22%7D%5D%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22798b8b1b-c88d-42e8-91f8-247fd4282fe7%22%7D%2C%22layout%22:%224panel%22%7D)\n",
    "\n",
    "<div style=\"background:#e9e2f8; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:25px 0 5px 0px;\n",
    "color:#000000;\n",
    "width:fit-content; \">\n",
    "    <div style=\"font-weight:bold;\">üß† Learn more</div>\n",
    "    <div style=\"margin-top:2px;\">\n",
    "        For readers interested in exploring more tools compatible with DANDI-supported data formats, refer to:\n",
    "    </div>\n",
    "    <ul style=\"margin-top:5px; margin-bottom:5px;\">\n",
    "        <li><a href=\"https://nwb-overview.readthedocs.io/en/latest/tools/analysis_tools_home.html#analysis-and-visualization-tools\" style=\"font-weight:bold;\">NWB: Analysis Tools</a></li>\n",
    "        <li><a href=\"https://bids.neuroimaging.io/tools/others.html#analysis\" style=\"font-weight:bold;\">BIDS: Analysis Tools</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "*The difference between data formats and standards is elaborated in greater detail in the [Data\n",
    "Standards](https://docs.dandiarchive.org/getting-started/data-standards/#data-standards) section of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362bd15",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal of this section is to demonstrate loading a portion of data from your dataset, and reveal something about its structure.\n",
    "1. Load an object from S3\n",
    "2. Show the structure of data in the object\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: How do I access the contents of a Dandiset?\n",
    "\n",
    "Data contents from assets on the DANDI archive can either be downloaded or streamed directly from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up a specific file asset from a different Dandiset\n",
    "dandiset = client.get_dandiset(dandiset_id=\"000728\")\n",
    "dandi_filename = \"sub-491604983/sub-491604983_ses-501560436-StimC_behavior+image+ophys.nwb\"\n",
    "asset = dandiset.get_asset_by_path(path=dandi_filename)\n",
    "\n",
    "# Download entire file (alter the base directory as needed)\n",
    "output_path = Path.cwd() / Path(dandi_filename).name\n",
    "asset.download(filepath=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686af22a",
   "metadata": {},
   "source": [
    "To open the file after the download completes, we can use the [PyNWB](https://pynwb.readthedocs.io/en/stable/)\n",
    "library to read the NWB file and display the basic content layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbfile = read_nwb(path=output_path)\n",
    "print(nwbfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622f1ba6d3c3d43",
   "metadata": {},
   "source": [
    "When running the notebook in compatible environments (_e.g._, Jupyter), you can also interact with the filetree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b6d22688d5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9062e1",
   "metadata": {},
   "source": [
    "Specific data arrays can be accessed by traversing the NWB file structure.\n",
    "\n",
    "For example, the $\\Delta F/F$ time series derived from the raw two-photon calcium imaging can be found under the\n",
    "'processing' module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7209334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_f_array = nwbfile.processing[\"ophys\"][\"DfOverF\"][\"DfOverF\"].data\n",
    "\n",
    "# Get a subset of the data for visualization\n",
    "# Note that the `df_over_f_array` has shape `number of frames x number of regions of interest`\n",
    "# reflecting dimensions of `time x ROIs`\n",
    "time_series_data = df_over_f_array[:1000, :5]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(time_series_data.shape[1]):\n",
    "    plt.plot(time_series_data[:, i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('ŒîF/F')\n",
    "plt.title('Calcium Imaging Time Series (ŒîF/F)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8260c9e1292192a",
   "metadata": {},
   "source": [
    "Note that all data access when reading from an NWB file is 'lazy' in the sense that data arrays are not read into memory\n",
    "until explicitly requested via slicing operations.\n",
    "\n",
    "This is particularly useful when working with large (> 60 GB) datasets that\n",
    "may not otherwise fit into memory.\n",
    "\n",
    "Additionally, some files on the DANDI archive can be quite large (up to TB-size files in multi-TB Dandisets)!\n",
    "\n",
    "Instead of downloading these, you can stream data directly from S3 using any of the [libraries supported by PyNWB](https://pynwb.readthedocs.io/en/stable/tutorials/advanced_io/streaming.html).\n",
    "\n",
    "The previous command can be adapted to stream directly from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c30a8f2dca6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "rem_file = remfile.File(url=s3_url)\n",
    "h5py_file = h5py.File(name=rem_file, mode=\"r\")\n",
    "io = NWBHDF5IO(file=h5py_file)\n",
    "streamed_nwbfile = io.read()\n",
    "\n",
    "streamed_df_over_f_array = streamed_nwbfile.processing[\"ophys\"][\"DfOverF\"][\"DfOverF\"].data\n",
    "\n",
    "streamed_time_series_data = streamed_df_over_f_array[:1000, :5]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(streamed_time_series_data.shape[1]):\n",
    "    plt.plot(streamed_time_series_data[:, i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('ŒîF/F')\n",
    "plt.title('Calcium Imaging Time Series (ŒîF/F)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da35825a98e0d0",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal here is to visualize some aspect of your dataset in order to help users understand it. In addition to helping users of your dataset understand the dataset, an additional goal is to impress!\n",
    "\n",
    "Please demonstrate any data preprocessing or reshaping required for your visualization(s).\n",
    "\n",
    "https://www.reddit.com/r/dataisbeautiful/ for inspiration.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: A picture is worth a thousand words. Show us a visual (or several!) from your dataset that either illustrates something informative about your dataset, or that you think might excite someone to dig in further.\n",
    "\n",
    "Using the same Dandiset as the previous section, we can visualize the connection between the segmentation and the\n",
    "cellular imaging by overlying the summary image with the ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a34845447fca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_image = nwbfile.processing[\"ophys\"][\"SummaryImages\"][\"mean_image\"].data\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(summary_image[:], cmap='gray')\n",
    "plt.title('Mean Summary Image of Imaging Field')\n",
    "plt.xlabel('X (pixels)')\n",
    "plt.ylabel('Y (pixels)')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375249105fd33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_image = nwbfile.processing[\"ophys\"][\"SummaryImages\"][\"maximum_intensity_projection\"]\n",
    "summary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422141d366b519c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\"PlaneSegmentation\"]\n",
    "\n",
    "# TODO: cody finish making pretty overlay of ROI to max proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a4ff08179a91b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T18:48:20.860160100Z",
     "start_time": "2026-01-29T18:48:20.848519400Z"
    }
   },
   "source": [
    "The examples above showcase optophysiology data, but DANDI hosts diverse neurophysiology modalities.\n",
    "\n",
    "Let's explore some other data types - such as electrophysiology!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab3bf056c8a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Heberto waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d071437d215bc7",
   "metadata": {},
   "source": [
    "DANDI doesn't just host neural data, either - it is quite common for Dandisets to include behavioral data as well.\n",
    "\n",
    "Let's take a look at one of the most common behavioral data types - pose estimated video!\n",
    "\n",
    "### The International Brain Laboratory and Standardized Behavior\n",
    "\n",
    "The [International Brain Laboratory (IBL)](https://www.internationalbrainlab.com/) is a collaboration of over 20 neuroscience laboratories working together to understand how the brain produces decisions. One of their key innovations is the use of **standardized behavioral protocols** across all participating laboratories.\n",
    "\n",
    "In each IBL experiment, a mouse performs a visually-guided decision-making task while being recorded by three synchronized cameras:\n",
    "- **Left camera**: captures facial features and left paw movements\n",
    "- **Right camera**: captures facial features and right paw movements  \n",
    "- **Body camera**: records the animal's posture from above\n",
    "\n",
    "Rather than relying on physical markers attached to the animal, IBL uses [DeepLabCut](https://www.mackenziemathislab.org/deeplabcut) and [Lightning Pose](https://lightning-pose.readthedocs.io/) for markerless pose estimation. These deep learning methods automatically track anatomical landmarks (like paws, nose, tongue, and pupil) across video frames.\n",
    "\n",
    "We can showcase how to load and visualize this pose estimation data using an example NWB file from the IBL Dandiset using the `nwb-video-widgets` library.\n",
    "\n",
    "The IBL data is available on DANDI as [Dandiset 000409](https://dandiarchive.org/dandiset/000409) - \"IBL - Brain Wide Map\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3wzlehxqsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the nwb-video-widgets package for interactive pose visualization\n",
    "!pip install -q nwb-video-widgets[dandi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a64ls00j2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwb_video_widgets import NWBDANDIPoseEstimationWidget\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "# Connect to DANDI and get the IBL Dandiset\n",
    "client = DandiAPIClient()\n",
    "dandiset = client.get_dandiset(\"000409\", \"draft\")\n",
    "\n",
    "# IBL session with pose estimation data\n",
    "session_eid = \"64e3fb86-928c-4079-865c-b364205b502e\"\n",
    "\n",
    "# Find assets for this session\n",
    "session_assets = [asset for asset in dandiset.get_assets() if session_eid in asset.path]\n",
    "raw_asset = next((asset for asset in session_assets if \"desc-raw\" in asset.path), None)\n",
    "processed_asset = next((asset for asset in session_assets if \"desc-processed\" in asset.path), None)\n",
    "\n",
    "# Display pose estimation widget - streams video from S3 and overlays keypoints\n",
    "NWBDANDIPoseEstimationWidget(\n",
    "    asset=processed_asset,\n",
    "    video_asset=raw_asset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvloej7slij",
   "metadata": {},
   "source": [
    "The widget above demonstrates how behavioral data in NWB format can be visualized alongside neural recordings. Each colored dot represents a tracked body part, with coordinates extracted frame-by-frame by the pose estimation model.\n",
    "\n",
    "This integration of video, pose estimation, and neural data in a single standardized format (NWB) is a key feature that makes DANDI datasets suitable for studying brain-behavior relationships at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c8b85-ed1c-4f2c-bd0e-fbfbc67c4723",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is less prescriptive / freeform than previous sections. The goal here is to show an opinionated example of answering a question using your data. The scale of your dataset may preclude a full example, and so feel free to limit the scope of this example (e.g. work on a subset of data). Users should be able to replicate your example in this notebook, and get a sense of how they would scale up.\n",
    "\n",
    "A \"toy\" example is better than no example.\n",
    "\n",
    "Ideally, your example would:\n",
    "1. Transmit some of your domain & dataset experience to the reader, drawing on your own work as much as possible\n",
    "2. Provide a jumping off point for users to extend your work, and do novel work of their own.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one question that you have answered using these data? Can you show us how you came to that answer?\n",
    "\n",
    "Focusing on the optophysiology example used above - the Visual Coding project by the Allen Institute - one question\n",
    "that was addressed involves characterizing population-level response characteristics across\n",
    "visual cortex.\n",
    "\n",
    "\n",
    "Multiple stimuli (natural scenes, drifting gratings, static gratings) were presented to each subject over the course\n",
    "of the experiment. Different structures within the visual cortex were targeted across subjects. The neural responses during each presentation were then characterized to\n",
    " show differing response properties across visual areas. This demonstrated that different cortical layers have distinct\n",
    " response properties and tuning characteristics. The experiments also quantified how correlated activity between\n",
    " neurons affects information coding by showed that noise correlations are stronger between neurons with similar tuning\n",
    " properties. Additional findings demonstrate that correlations are modulated by behavioral state (running vs.\n",
    " stationary movements).\n",
    "\n",
    "A full reproducible analysis of this work can be found through its very detailed [example notebook](https://github.com/dandi/example-notebooks/blob/master/000728/AllenInstitute/visual_coding_ophys_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf645724-3108-4ada-a832-10b3431eb8e2",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is, like the previous one, intended to be freeform / non-prescriptive. The goal here is to provide a challenge to the community to do something novel with your dataset. That can either be novel in terms of the task, or novel in terms of methodological or computational approach.\n",
    "\n",
    "Another way to consider this section, is as a wishlist. If you were less constrained by time, cost, skill, etc., what would you like to see achieved using these data? \n",
    "\n",
    "The challenge should, however, be somewhat realistic. A challenge that assumes e.g. original data collection, is likely to go unanswered.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one unanswered question that you think could be answered using these data? Do you have any recommendations or advice for someone wanting to answer this question?\n",
    "\n",
    "One such proposal might be how do different visual cortical areas (V1, LM, AL, PM) coordinate their activity over time\n",
    "during naturalistic scene viewing, and can we identify temporal \"routing\" patterns that predict behavioral state transitions?\n",
    "\n",
    "While the Visual Coding dataset(s) have characterized individual area responses, differences across cell types, and\n",
    "other correlations, the temporal dynamics of information flow between areas during natural scene processing remains\n",
    "less explored - particularly how running vs. stationary states modulate inter-area communication.\n",
    "\n",
    "It is worth mentioning in this context that the NWB group hosts a regular [NeuroDataReHack event](https://nwb.org/events/hck26-2026-janelia-ndrh/)\n",
    "where researchers are brought together to work precisely on such questions of how to analyze existing datasets in\n",
    "novel ways, rather than running entirely new experiments. Check the [NWB Events](https://nwb.org/events/) page and\n",
    "sign up for the newsletter to stay informed about these kinds of events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c878722b2cc2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "know_your_dandiset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
