{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "# Exploring the DANDI Archive\n",
    "\n",
    "This notebook serves as a quick-start guide for the [Distributed Archive for Neurophysiology Data Integration (DANDI)](https://registry.opendata.aws/dandiarchive/).\n",
    "\n",
    "<div style=\"background:#fff2cc; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:-5px 0 5px 20px;\n",
    "color:#000000;\n",
    "display:inline-block; \">\n",
    "    <div style=\"font-weight:bold;\">‚ÑπÔ∏è Definition</div>\n",
    "    A <i>Dandiset</i> is a collection of neurophysiology data and metadata hosted on the <a\n",
    "    href=\"https://dandiarchive.org\" style=\"font-weight:bold;\">DANDI Archive</a>.\n",
    "</div>\n",
    "\n",
    "The DANDI Archive holds hundreds of Dandisets with a diverse range of neurodata modalities.\n",
    "\n",
    "These modalities span the spectrum of microscopy, optogenetics, intracellular and extracellular\n",
    "electrophysiology, and optophysiology.\n",
    "\n",
    "While we cannot hope to completely showcase this diversity here, there are two key examples which provide a good\n",
    "starting point:\n",
    "- [000728 - Visual Coding - Optical Physiology](https://dandiarchive.org/dandiset/000728/) by the Allen Institute for\n",
    "Brain Science (AIBS)\n",
    "- [000409 - Brain Wide Map](https://dandiarchive.org/dandiset/000409/) by the International Brain Laboratory (IBL)\n",
    "\n",
    "For even more [usage guides](https://docs.dandiarchive.org/user-guide-using/exploring-dandisets/),\n",
    "[dandiset-specific tutorials](https://dandi.github.io/example-notebooks/), and general documentation, please read the\n",
    "main [DANDI Docs](https://docs.dandiarchive.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779654-eeee-4708-83cf-245e03303475",
   "metadata": {},
   "source": [
    "### Q: How do I navigate the archive and its datasets?\n",
    "\n",
    "DANDI provides a [web interface](https://dandiarchive.org/dandiset), [REST API](https://api.dandiarchive.org/api/docs/swagger/),\n",
    "and [command-line interface (CLI)](https://pypi.org/project/dandi/) to help users intuitively navigate the contents.\n",
    "\n",
    "The easiest place to start is the primary [Dandiset listing page](https://dandiarchive.org/dandiset).\n",
    "\n",
    "After scrolling around a while, we choose our first Dandiset from the web interface [000728](https://dandiarchive.org/dandiset/000728/0.240827.1809).\n",
    "\n",
    "We can see the contents by going to the [\"Files\" tab](https://dandiarchive.org/dandiset/000728/0.240827.1809/files).\n",
    "\n",
    "From here, we can see that a Dandiset is organized as a collection of folders organized by subject ID.\n",
    "\n",
    "Each folder contains files named according to session ID or other unique discriminators:\n",
    "\n",
    "```text\n",
    "000728/\n",
    "‚îú‚îÄ‚îÄ sub-691657859/sub-691657859_ses-712919679-StimB_ophys.nwb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sub-691657859_ses-712919679-StimB_ophys.nwb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sub-691657859_ses-710504563-StimA_behavior+image+ophys.nwb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ sub-501800590/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b47b69",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before we start accessing data contents, we will need to install and import some Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e65803f0",
   "metadata": {},
   "source": [
    "!pip install -q dandi matplotlib remfile opencv-python-headless\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import remfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "from pynwb import read_nwb, NWBHDF5IO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b14ae10",
   "metadata": {},
   "source": [
    "Next, we will initialize our DANDI API client to interact with the archive database and list a few of the\n",
    "available Dandisets:"
   ]
  },
  {
   "cell_type": "code",
   "id": "be33d211",
   "metadata": {},
   "source": [
    "client = DandiAPIClient()\n",
    "dandisets = list(client.get_dandisets())\n",
    "\n",
    "# Print the dandiset IDs and titles of the first 3 dandisets\n",
    "for dandiset in dandisets[:3]:\n",
    "    print(f\"{dandiset.identifier}: {dandiset.get_raw_metadata()[\"name\"]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "efb9fa4d",
   "metadata": {},
   "source": [
    "Now let's return to our first example Dandiset and list out a few of its contents:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c582a4ce",
   "metadata": {},
   "source": [
    "dandiset = client.get_dandiset(dandiset_id=\"000728\", version_id=\"0.240827.1809\")\n",
    "assets = list(dandiset.get_assets())\n",
    "\n",
    "# Print the file paths as seen on the DANDI web interface\n",
    "for asset in assets[:3]:\n",
    "    print(asset.get_raw_metadata()[\"path\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47268012",
   "metadata": {},
   "source": [
    "<div style=\"background:#cfe2f3; border:1px solid #000000; border-radius:2px; padding:2px 5px; color:#000000;\n",
    "display:inline-block;\">\n",
    "    <div style=\"font-weight:bold;\">üí° Info</div>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "        Notice that we passed a <code style=\"background:#cfe2f3; color:#000000\">version_id</code> in this case.\n",
    "        Dandisets that are published on the archive are given a citable DOI, such as:\n",
    "    </div>\n",
    "    <div style=\"padding-left:20px; margin:5px 0;\">\n",
    "        Allen Institute (2024) <em>Allen Institute - Visual Coding - Optical Physiology</em> (Version 0.240827.1809)\n",
    "        [Data set]. DANDI archive.<br><a href=\"https://doi.org/10.48324/dandi.000728/0.240827.1809\"\n",
    "        style=\"font-weight:bold;\">DOI: 10.48324/dandi.000728/0.240827.1809</a>\n",
    "    </div>\n",
    "    <div>These citations should be used in any scientific reuse of the data.</div>\n",
    "    <div style=\"margin-top:5px;\">\n",
    "        Otherwise, the most recent 'draft' state of the Dandiset is used by default and is subject to change by the Dandiset contributors.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd7a3a2b42a29f",
   "metadata": {},
   "source": [
    "<div style=\"background:#e9e2f8; border:1px solid #000000; border-radius:2px; padding:2px 5px; color:#000000;\n",
    "display:inline-block; \">\n",
    "    <div style=\"font-weight:bold;\">üß† Learn more</div>\n",
    "    You may have also noticed that in several cases above, we fetched the metadata associated with the Dandisets and\n",
    "    their assets.\n",
    "    <br>\n",
    "    These are very rich models whose full potential is best showcased in the <a href=\"https://docs.dandiarchive\n",
    "    .org/example-notebooks/tutorials/cosyne_2023/advanced_asset_search/#going-beyond\" style=\"font-weight:bold;\">Advanced Search Tutorial</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bcf-ec40-432f-a31f-4477efa205ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q: What kinds of data are hosted and what formats do they use?\n",
    "\n",
    "DANDI accepts a relatively small number of open, community-driven file formats designed according to NIH-accepted\n",
    "data standards*.\n",
    "\n",
    "<div style=\"display: block; width: 100%;\">\n",
    "<div style=\"margin-left: 0; margin-right: auto; display: table;\">\n",
    "\n",
    "|                                         <center>Data Standard</center>                                          | <center>Acronym</center>  |                 <center>Domain</center>                  |    <center>Data Format(s)</center>    |\n",
    "|:---------------------------------------------------------------------------------------------------------------:|:-------------------------:|:--------------------------------------------------------:|:-------------------------------------:|\n",
    "|                         <center>[Neurodata Without Borders](https://nwb.org/)</center>                          |   <center>NWB</center>    |     <center>Neurophysiology<br>and behavior</center>     |     <center>HDF5<br>Zarr</center>     |\n",
    "|                 <center>[Brain Imaging Data Structure](https://bids.neuroimaging.io/)</center>                  |   <center>BIDS</center>   |    <center>Neuroimaging<br>(MRI, EEG, etc.)</center>     | <center>NIfTI<br>JSON<br>TSV</center> |\n",
    "|    <center>[Open Microscopy Environment](https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/)</center>    | <center>OME-TIFF</center> |           <center>Microscopy imaging</center>            |         <center>TIFF</center>         |\n",
    "| <center>[Open Microscopy Environment<br>Next Generation File Format](https://ngff.openmicroscopy.org/)</center> | <center>OME-Zarr</center> | <center>Microscopy imaging<br>(cloud-optimized)</center> |         <center>Zarr</center>         |\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "These data standards are specifically designed to integrate multi-modal raw and processed neurodata alongside\n",
    "behavioral data and metadata annotations.\n",
    "\n",
    "The S3 bucket hosting the DANDI archive allows users to take advantage of cloud-native\n",
    "services for scalable data access, computation, visualization, and analysis.\n",
    "\n",
    "This allows DANDI to integrate with many external visualization tools, accessible via the \"Open With\" button on the\n",
    "web interface.\n",
    "\n",
    "- [NWB: Neurosift](https://neurosift.app)\n",
    "    - [Example: 000728/sub-495727000/sub-495727000_ses-51254258-StimC_behavior+image+ophys.nwb](https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/0205b9b1-10c4-467c-b027-20bbbfcce3a0/download/&dandisetId=001172&dandisetVersion=0.260129.0829)\n",
    "- [OME: Neuroglancer](https://github.com/google/neuroglancer)\n",
    "    - [Example: 000026/sub-I58/ses-Hip-CT/micr/](https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22z%22:%5B0.00001513%2C%22m%22%5D%2C%22y%22:%5B0.00001513%2C%22m%22%5D%2C%22x%22:%5B0.00001513%2C%22m%22%5D%7D%2C%22position%22:%5B5257.03564453125%2C4706%2C4218.56396484375%5D%2C%22crossSectionScale%22:21.48356465187443%2C%22projectionScale%22:16384%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22https://dandiarchive.s3.amazonaws.com/zarr/5c37c233-222f-4e60-96e7-a7536e08ef61%22%2C%22tab%22:%22rendering%22%2C%22shaderControls%22:%7B%22normalized%22:%7B%22range%22:%5B23257%2C24764%5D%2C%22window%22:%5B22877%2C25144%5D%7D%7D%2C%22name%22:%22798b8b1b-c88d-42e8-91f8-247fd4282fe7%22%7D%5D%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22798b8b1b-c88d-42e8-91f8-247fd4282fe7%22%7D%2C%22layout%22:%224panel%22%7D)\n",
    "\n",
    "<div style=\"background:#e9e2f8; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:25px 0 5px 0px;\n",
    "color:#000000;\n",
    "display:inline-block; \">\n",
    "    <div style=\"font-weight:bold;\">üß† Learn more</div>\n",
    "    <div style=\"margin-top:2px;\">\n",
    "        For readers interested in exploring more tools compatible with DANDI-supported data formats, refer to:\n",
    "    </div>\n",
    "    <ul style=\"margin-top:5px; margin-bottom:5px;\">\n",
    "        <li><a href=\"https://nwb-overview.readthedocs.io/en/latest/tools/analysis_tools_home.html#analysis-and-visualization-tools\" style=\"font-weight:bold;\">NWB: Analysis Tools</a></li>\n",
    "        <li><a href=\"https://bids.neuroimaging.io/tools/others.html#analysis\" style=\"font-weight:bold;\">BIDS: Analysis Tools</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "*The difference between data formats and standards is elaborated in greater detail in the [Data\n",
    "Standards](https://docs.dandiarchive.org/getting-started/data-standards/#data-standards) section of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362bd15",
   "metadata": {},
   "source": [
    "### Q: How do I access the contents of a Dandiset?\n",
    "\n",
    "Data assets from a Dandiset can either be downloaded directly [from the web page](https://docs.dandiarchive.org/user-guide-using/accessing-data/downloading/#download-specific-files), [through the\n",
    "CLI](https://docs.dandiarchive.org/user-guide-using/accessing-data/downloading/#using-the-python-cli-client),\n",
    "or programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "id": "963d3cff",
   "metadata": {},
   "source": [
    "# Look up a specific file asset from a different Dandiset\n",
    "dandiset = client.get_dandiset(dandiset_id=\"000728\")\n",
    "dandi_filename = \"sub-495727015/sub-495727015_ses-501559087-StimB_behavior+image+ophys.nwb\"\n",
    "asset = dandiset.get_asset_by_path(path=dandi_filename)\n",
    "\n",
    "# Download the entire file (alter the base directory as needed)\n",
    "output_path = Path.cwd() / Path(dandi_filename).name\n",
    "if not output_path.exists():\n",
    "    asset.download(filepath=output_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "686af22a",
   "metadata": {},
   "source": [
    "To open the file after the download completes, we can use the [PyNWB](https://pynwb.readthedocs.io/en/stable/)\n",
    "library to read the NWB file and display the basic content layout:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed25fd6e",
   "metadata": {},
   "source": [
    "nwbfile = read_nwb(path=output_path)\n",
    "print(nwbfile)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a622f1ba6d3c3d43",
   "metadata": {},
   "source": [
    "<div style=\"background:#cfe2f3; border:1px solid #000000; border-radius:2px; padding:2px 5px;\n",
    "color:#000000; display:inline-block;\">\n",
    "    <div style=\"font-weight:bold;\">üí° Info</div>\n",
    "    In this type of experiment, a location in the brain is recorded by a microscope in real time while the neurons or\n",
    "    other cells emit light corresponding to their activity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad662cd9716d92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T02:54:51.554029400Z",
     "start_time": "2026-02-10T02:54:51.519088100Z"
    }
   },
   "source": [
    "<div style=\"background:#e9e2f8; border:1px solid #000000; border-radius:2px; padding:2px 5px;color:#000000;display:inline-block; \">\n",
    "    <div style=\"font-weight:bold;\">üß† Learn more</div>\n",
    "    <div style=\"margin-top:2px;\">\n",
    "        Here, the molecule that emits light is a calcium indicator, which fluctuates in proportion to the number of\n",
    "        action potentials fired by the neurons.\n",
    "        <br><br>\n",
    "        If you are interested in learning more about two-photon calcium imaging, or have any questions about the rest\n",
    "        of this experiment, check out the <a href=\"https://alleninstitute.github.io/openscope_databook/basics/background.html#understanding-data-collection-techniques\" style=\"font-weight:bold;\">OpenScope Databook</a>.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9062e1",
   "metadata": {},
   "source": [
    "A common way of analyzing fluorescence imaging data is to quantify the change in the amount of light emitted by\n",
    "specific regions.\n",
    "\n",
    "One such measure is the $\\Delta F/F$ time series, which is derived from the raw two-photon calcium imaging.\n",
    "\n",
    "This data stream can be found under the 'processing' module:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7209334",
   "metadata": {},
   "source": [
    "df_over_f_array = nwbfile.processing[\"ophys\"][\"DfOverF\"][\"DfOverF\"].data\n",
    "\n",
    "# Get a subset of the data for visualization\n",
    "# Note that the `df_over_f_array` has shape `number of frames x number of regions of interest (ROI)`\n",
    "# reflecting the dimensions of `time x ROIs`\n",
    "time_series_data = df_over_f_array[:1000, :5]\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "for i in range(time_series_data.shape[1]):\n",
    "    plt.plot(time_series_data[:, i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('ŒîF/F')\n",
    "plt.title('Calcium Imaging Time Series (ŒîF/F)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8260c9e1292192a",
   "metadata": {},
   "source": [
    "<div style=\"background:#cfe2f3; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:5px 0 5px 0px;\n",
    "color:#000000; display:inline-block;\">\n",
    "    <div style=\"font-weight:bold;\">üí° Info</div>\n",
    "    All data access when reading from an NWB file is 'lazy' in the sense that data arrays are not read into memory\n",
    "    until explicitly requested via slicing operations.\n",
    "    <br>\n",
    "    This is particularly useful when working with large (> 60 GB) datasets that may not otherwise fit into\n",
    "    memory.\n",
    "</div>\n",
    "\n",
    "Some files on the DANDI archive can be quite large - even hundreds of gigabytes - which makes downloading a\n",
    "file just to explore its contents impractical.\n",
    "\n",
    "Thankfully, instead of downloading, you can [stream data directly from S3](https://docs.dandiarchive.org/example-notebooks/tutorials/bcm_2024/analysis-demo/#streaming-and-interacting-with-nwb-data-from-dandi)!\n",
    "\n",
    "Let's give that a try:"
   ]
  },
  {
   "cell_type": "code",
   "id": "30c30a8f2dca6443",
   "metadata": {},
   "source": [
    "s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "rem_file = remfile.File(url=s3_url)\n",
    "h5py_file = h5py.File(name=rem_file, mode=\"r\")\n",
    "io = NWBHDF5IO(file=h5py_file)\n",
    "streamed_nwbfile = io.read()\n",
    "\n",
    "streamed_df_over_f_array = streamed_nwbfile.processing[\"ophys\"][\"DfOverF\"][\"DfOverF\"].data\n",
    "\n",
    "streamed_time_series_data = streamed_df_over_f_array[:1000, :5]\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "for i in range(streamed_time_series_data.shape[1]):\n",
    "    plt.plot(streamed_time_series_data[:, i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('ŒîF/F')\n",
    "plt.title('Calcium Imaging Time Series (ŒîF/F)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60da35825a98e0d0",
   "metadata": {},
   "source": [
    "While we just showcased a simple data array, DANDI assets can also include beautiful images and videos!\n",
    "\n",
    "Now let's get a better understanding of how the identified regions of interest relate to our underlying imaging data.\n",
    "\n",
    "These data streams can be accessed and displayed in nearly the same manner:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a34845447fca7f",
   "metadata": {},
   "source": [
    "summary_image = nwbfile.processing[\"ophys\"][\"SummaryImages\"][\"maximum_intensity_projection\"][:]\n",
    "plane_segmentation = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\"PlaneSegmentation\"]\n",
    "\n",
    "combined_image_masks = np.zeros(shape=summary_image.shape)\n",
    "for pixel_mask in plane_segmentation[\"pixel_mask\"][:]:\n",
    "    for x, y, w in pixel_mask:\n",
    "        combined_image_masks[x,y] += w\n",
    "masked_image = np.ma.masked_where(combined_image_masks == 0, combined_image_masks)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(summary_image, cmap=\"gray\")\n",
    "plt.imshow(masked_image, cmap=\"viridis\", alpha=0.5)\n",
    "plt.axis('off');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e423a9aa93d4d834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:30:52.811979600Z",
     "start_time": "2026-02-04T17:30:52.764406200Z"
    }
   },
   "source": [
    "The gray background represents the imaging space, while the colored overlay indicates the\n",
    "identified regions where neural activity was measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef319169b120fa",
   "metadata": {},
   "source": [
    "As mentioned, DANDI hosts a diverse range of neurophysiology data modalities - not just optophysiology!\n",
    "\n",
    "Let's also showcase some electrophysiology and behavioral data:"
   ]
  },
  {
   "cell_type": "code",
   "id": "7eab3bf056c8a333",
   "metadata": {},
   "source": [
    "# Setup streaming like we did before\n",
    "ecephys_dandiset = client.get_dandiset(\"000409\", \"draft\")\n",
    "\n",
    "subject_id = \"sub-NYU-39\"\n",
    "session_id = \"ses-6ed57216-498d-48a6-b48b-a243a34710ea\"\n",
    "ecephys_path = f\"{subject_id}/{subject_id}_{session_id}_desc-processed_behavior+ecephys.nwb\"\n",
    "ecephys_asset = ecephys_dandiset.get_asset_by_path(path=ecephys_path)\n",
    "\n",
    "ecephys_s3_url = ecephys_asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "ecephys_rem_file = remfile.File(url=ecephys_s3_url)\n",
    "ecephys_h5py_file = h5py.File(name=ecephys_rem_file, mode=\"r\")\n",
    "ecephys_io = NWBHDF5IO(file=ecephys_h5py_file)\n",
    "ecephys_nwbfile = ecephys_io.read()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2edd1c5",
   "metadata": {},
   "source": [
    "# Filter by 'good units' (those with non-NaN waveforms) and select one for visualization\n",
    "units_dataframe = ecephys_nwbfile.units.to_dataframe()\n",
    "good_unit = units_dataframe[\n",
    "    [not np.isnan(row[\"waveform_mean\"]).any() for _, row in units_dataframe.iterrows()]\n",
    "].iloc[10]\n",
    "\n",
    "# Extract waveform and convert from volts to microvolts\n",
    "waveform_uV = good_unit[\"waveform_mean\"] * 1e6\n",
    "trace = waveform_uV[:, 19]\n",
    "\n",
    "# Define the time axes of the waveform\n",
    "num_samples = waveform_uV.shape[0]\n",
    "sampling_rate = 30_000.0\n",
    "time_ms = (np.arange(num_samples) - num_samples // 2) / sampling_rate * 1000\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "ax.plot(time_ms, trace, color=\"black\", linewidth=1.0)\n",
    "ax.fill_between(time_ms, 0, trace, where=(trace > 0), color=\"orange\", alpha=0.8)\n",
    "ax.fill_between(time_ms, 0, trace, where=(trace < 0), color=\"slateblue\", alpha=0.8)\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Time (ms)\")\n",
    "ax.set_ylabel(\"Amplitude (uV)\")\n",
    "plt.title(f\"Average waveform of a spiking unit\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5d071437d215bc7",
   "metadata": {},
   "source": [
    "<div style=\"background:#e9e2f8; border:1px solid #000000; border-radius:2px; padding:2px 5px; margin:25px 0 5px 0px;\n",
    "color:#000000;\n",
    "display:inline-block; \">\n",
    "    <div style=\"font-weight:bold;\">üß† Learn more</div>\n",
    "    <div style=\"margin-top:2px;\">\n",
    "        In this type of experiment, one or more <a href=\"https://www.neuropixels.org/\" style=\"font-weight:bold;\n",
    "        \">Neuropixels probes</a> were inserted\n",
    "        into specific locations within the brain to record the electrical activity around clusters of neurons.\n",
    "        <br><br>\n",
    "        If you are interested in learning more about this experiment, check out the <a href=\"https://www\n",
    "        .internationalbrainlab.com/brainwide-map\"\n",
    "         style=\"font-weight:bold;\">International Brain Lab: The Brain-Wide Map</a> website.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f433bab82589",
   "metadata": {},
   "source": [
    "Alongside the recordings of neural activity, a video captures the animal performing the simple task of turning a wheel!\n",
    "\n",
    "Various points on the body (such as hands and eyes) are then tracked using a technique known as 'pose estimation':"
   ]
  },
  {
   "cell_type": "code",
   "id": "7de3157b46e53bee",
   "metadata": {},
   "source": [
    "video_directory = f\"{subject_id}/{subject_id}_{session_id}_behavior+ecephys+image\"\n",
    "video_path = f\"{video_directory}/{subject_id}_{session_id}_OriginalVideoLeftCamera.mp4\"\n",
    "video_asset = ecephys_dandiset.get_asset_by_path(path=video_path)\n",
    "video_s3_url = video_asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_s3_url)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 100000)\n",
    "_, frame = cap.read()\n",
    "\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "pose_estimation_module = ecephys_nwbfile.processing[\"pose_estimation\"]\n",
    "left_camera_pose_estimation = pose_estimation_module[\"LeftCamera\"]\n",
    "pose_estimation_series = left_camera_pose_estimation.pose_estimation_series\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.imshow(frame_rgb)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(pose_estimation_series)))\n",
    "for color, keypoint in zip(colors, pose_estimation_series):\n",
    "    series = pose_estimation_series[keypoint]\n",
    "    first_frame_point = series.data[0, :]\n",
    "    plt.scatter(\n",
    "        first_frame_point[0],\n",
    "        first_frame_point[1],\n",
    "        label=series.name.replace(\"PoseEstimationSeries\", \"\"),\n",
    "        color=color,\n",
    "        s=40,\n",
    "        zorder=5,\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize=7, framealpha=0.8, ncol=2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "183c8b85-ed1c-4f2c-bd0e-fbfbc67c4723",
   "metadata": {},
   "source": [
    "### Q: What is one scientific question that has been answered using these data?\n",
    "\n",
    "Focusing on the optophysiology example used above - the Visual Coding project by the Allen Institute - one question\n",
    "that was addressed involves characterizing population-level response characteristics across\n",
    "visual cortex.\n",
    "\n",
    "\n",
    "Multiple stimuli (natural scenes, drifting gratings, static gratings) were presented to each subject over the course\n",
    "of the experiment. Different structures within the visual cortex were targeted across subjects. The neural responses during each presentation were then characterized to\n",
    " show differing response properties across visual areas. This demonstrated that different cortical layers have distinct\n",
    " response properties and tuning characteristics. The experiments also quantified how correlated activity between\n",
    " neurons affects information coding by showed that noise correlations are stronger between neurons with similar tuning\n",
    " properties. Additional findings demonstrate that correlations are modulated by behavioral state (running vs.\n",
    " stationary movements).\n",
    "\n",
    "A full reproducible analysis of this work can be found through its more detailed [tutorial notebook](https://github.com/dandi/example-notebooks/blob/master/000728/AllenInstitute/visual_coding_ophys_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf645724-3108-4ada-a832-10b3431eb8e2",
   "metadata": {},
   "source": [
    "### Q: What is one unanswered question that you think could be answered using these data?\n",
    "\n",
    "One such question might be how do different visual cortical areas (V1, LM, AL, PM) coordinate their activity over time\n",
    "during naturalistic scene viewing, and can we identify temporal \"routing\" patterns that predict behavioral state transitions?\n",
    "\n",
    "While the Visual Coding dataset(s) have characterized individual area responses, differences across cell types, and\n",
    "other correlations, the temporal dynamics of information flow between areas during natural scene processing remains\n",
    "less explored - particularly how running vs. stationary states modulate inter-area communication.\n",
    "\n",
    "This dataset is just one small part of the greater Allen Brain Observatory effort, which has seen considerable reuse\n",
    "based on questions like these. You can read more about this project in the following publication:\n",
    "\n",
    "> de Vries, S. E., Siegle, J. H., & Koch, C. (2023). Sharing neurophysiology data from the Allen Brain Observatory.\n",
    "> Elife, 12, e85550. DOI: https://doi.org/10.7554/eLife.85550\n",
    "\n",
    "It is worth mentioning in this context that the NWB group hosts a regular [NeuroDataReHack event](https://nwb.org/events/hck26-2026-janelia-ndrh/)\n",
    "where researchers are brought together to work precisely on such questions of how to analyze existing datasets in\n",
    "novel ways, rather than running entirely new experiments. Check the [NWB Events](https://nwb.org/events/) page and\n",
    "sign up for the newsletter to stay informed about these kinds of events!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:name-dandi+notebooks_date-2+2+26_py-3+12_env]",
   "language": "python",
   "name": "conda-env-name-dandi_notebooks_date-2_2_26_py-3_12_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
