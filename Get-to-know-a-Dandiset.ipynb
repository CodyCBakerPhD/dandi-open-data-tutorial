{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "\n",
    "1. Provide the name of your dataset, replacing the bracketed placeholder text.\n",
    "2. Update the Registry of Open Data landing page URL, by replacing the bracketed placeholder text. The [REGISTRY_YAML_NAME] will correspond to the name of the YAML document in your pull request to the Registry of Open Data on Github, minus the .yaml file extension.\n",
    "3. Remove these comment blocks when you have completed each section.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "# Get to Know a Dataset: DANDI Archive\n",
    "\n",
    "> **Def:** A _Dandiset_ is a collection of neurophysiology data and metadata hosted on the [DANDI Archive](https://dandiarchive.org/).\n",
    "\n",
    "This notebook serves as a guided tour of the [Distributed Archive for Neurophysiology Data Integration (DANDI)](https://registry.opendata.aws/dandiarchive/), which holds hundreds of Dandisets with neurodata modalities spanning the\n",
    "diverse fields of microscopy, optogenetics, intracellular and extracellular electrophysiology, and optophysiology. To\n",
    " showcase this diversity, we will demonstrate examples taken from several key Dandisets. For even more [usage guides](https://docs.dandiarchive.org/user-guide-using/exploring-dandisets/),\n",
    "[dataset-specific tutorials](https://dandi.github.io/example-notebooks/), and general documentation, check out the main\n",
    "[DANDI Docs](https://docs.dandiarchive.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779654-eeee-4708-83cf-245e03303475",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "\n",
    "The goal of this section is to orient users to the structure of your dataset. \n",
    "\n",
    "1. How are key prefixes and objects organized in your S3 bucket?\n",
    "2. What kinds of filetypes are represented in your dataset?\n",
    "3. Explain with text what users are expected to encounter, and then demonstrate with code the organizational framework you applied when creating your dataset.\n",
    "4. The responses to each question section are meant to be expanded or replaced as dictated by your dataset\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: How have you organized your S3 bucket? Help us understand the key prefix structure.\n",
    "\n",
    "DANDI provides a [web interface](https://dandiarchive.org/dandiset), [REST API](https://api.dandiarchive.org/api/docs/swagger/),\n",
    "and [command-line tool](https://pypi.org/project/dandi/) to help users intuitively navigate the contents.\n",
    "\n",
    "Let's start by choosing a random Dandiset from the web interface (for example, [000003](https://dandiarchive.org/dandiset/000003/0.250624.0409))\n",
    "and navigating to its [\"Files\" tab](https://dandiarchive.org/dandiset/000003/0.250624.0409/files).\n",
    "\n",
    "We can see that a Dandiset is organized as a collection of folders organized by subject ID.\n",
    "\n",
    "Each folder contains files named according to session ID or other unique discriminators.\n",
    "\n",
    "```text\n",
    "000003/\n",
    "├── sub-YutaMouse20/\n",
    "│   ├── sub-YutaMouse20_ses-YutaMouse20-140321_behavior+ecephys.nwb\n",
    "│   ├── sub-YutaMouse20_ses-YutaMouse20-140324_behavior+ecephys.nwb\n",
    "│   └── ...\n",
    "├── sub-YutaMouse23/\n",
    "│   └── ...\n",
    "└──...\n",
    "```\n",
    "\n",
    "\n",
    "At the lowest level of the S3 bucket, DANDI uses a sophisticated checksum-based object store to deduplicate assets.\n",
    "\n",
    "Every file uploaded to the archive is given a prefix structure in the S3 bucket similar to:\n",
    "\n",
    "```\n",
    "s3://dandiarchive/blobs/fa3/c57/fa3c5758-1e3e-4aeb-8d97-ad2e6032d8c8\n",
    "...\n",
    "s3://dandiarchive/blobs/<first three characters of ID>/<next three characters of ID>/<full ID>\n",
    "```\n",
    "\n",
    "which is not very human-readable. As such, we do not recommend navigating the S3 bucket directly.\n",
    "\n",
    "Instead, we recommend using the other methods to explore the archive, which maps the file contents to their\n",
    "underlying storage pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b47b69",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First we will install and import the Python libraries required throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "e65803f0",
   "metadata": {},
   "source": [
    "# This notebook requires the following additional libraries\n",
    "!pip install -q dandi matplotlib remfile\n",
    "\n",
    "# Import the libraries required for this notebook\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import remfile\n",
    "import matplotlib.pyplot as plt\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "from pynwb import read_nwb, NWBHDF5IO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b14ae10",
   "metadata": {},
   "source": [
    "Next, we will initialize our DANDI API client to interact with the archive database and list out a few of the\n",
    "available Dandisets."
   ]
  },
  {
   "cell_type": "code",
   "id": "be33d211",
   "metadata": {},
   "source": [
    "client = DandiAPIClient()\n",
    "dandisets = list(client.get_dandisets())\n",
    "\n",
    "# Print the dandiset IDs and titles of the first 3 dandisets\n",
    "for dandiset in dandisets[:3]:\n",
    "    print(f\"{dandiset.identifier}: {dandiset.get_raw_metadata()[\"name\"]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "efb9fa4d",
   "metadata": {},
   "source": "Now let's return to our first example Dandiset and list out a few of its contents."
  },
  {
   "cell_type": "code",
   "id": "c582a4ce",
   "metadata": {},
   "source": [
    "dandiset = client.get_dandiset(dandiset_id=\"000003\", version_id=\"0.250624.0409\")\n",
    "assets = list(dandiset.get_assets())\n",
    "\n",
    "# Print the file paths as seen on the DANDI web interface\n",
    "for asset in assets[:3]:\n",
    "    print(asset.get_raw_metadata()[\"path\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47268012",
   "metadata": {},
   "source": [
    "Notice that we passed a `version_id` in this case. Dandisets can (and should!) be published to create an immutable (and\n",
    "citable!) DOI.\n",
    "\n",
    "Otherwise, the most recent 'draft' state of the Dandiset is used by default and is subject to change\n",
    "by the Dandiset contributors.\n",
    "\n",
    "You may have also noticed that in several cases above, we fetched the metadata associated with the Dandisets and\n",
    "their assets.\n",
    "\n",
    "These are very rich models whose full potential is best showcased in the [Advanced Search Tutorial](https://docs.dandiarchive.org/example-notebooks/tutorials/cosyne_2023/advanced_asset_search/#going-beyond)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bcf-ec40-432f-a31f-4477efa205ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is meant to orient users of your dataset to the formats present in your dataset, particularly if your dataset includes formats that may be unfamiliar to a general data scientist audience. This section should include:\n",
    "\n",
    "1. Explanation of data format(s) (very common formats can be very briefly described, while less common\n",
    "   or domain specific formats should include more explanation as well as links to official documentation)\n",
    "2. Explanation of why the data format was chosen for your dataset\n",
    "3. Recommendations around software and tooling to work with this data format\n",
    "4. Explanation of any dataset-specific aspects to your usage of the format\n",
    "5. Description of AWS services that may be useful to users working with your data\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What data formats are present in your dataset? What kinds of data are stored using these formats? Can you give any advice for how you work with these data formats?\n",
    "\n",
    "DANDI accepts a relatively small number of open, community-driven file formats designed according to NIH-accepted\n",
    "data standards and optimized for cloud storage and access.\n",
    "\n",
    "The primary formats currently supported are:\n",
    "- **Data**: [HDF5](https://www.hdfgroup.org/solutions/hdf5/), [Zarr](https://zarr.dev/), [Tiff](https://www.fileformat.info/format/tiff/egff.htm)\n",
    "- **Metadata**: JSON, YAML, TSV\n",
    "- **Video**: AVI/MP4/WMV/MOV/FLV/MKV\n",
    "\n",
    "The primary data standards currently supported are:\n",
    "- [Neurodata Without Borders (NWB)](https://nwb.org/) for neurophysiology and behavior (supports both HDF5 and Zarr).\n",
    "- [Brain Imaging Data Structure (BIDS)](https://bids.neuroimaging.io/) for EEG and other modalities.\n",
    "- [OME-TIFF](https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/) and [OME-NGFF/Zarr](https://ngff.openmicroscopy.org/) for microscopy images.\n",
    "\n",
    "These data standards are specifically designed to integrate multi-modal raw and processed neurodata alongside\n",
    "behavioral data and metadata annotations.\n",
    "\n",
    "DANDI datasets are hosted in Amazon Web Services Open Data buckets, allowing users to take advantage of cloud-native\n",
    "services for scalable data access, computation, visualization, and analysis.\n",
    "\n",
    "This allows DANDI to integrate with many external visualization tools, accessible via the \"Open With\" button on the web interface:\n",
    "- [NWB: Neurosift](https://neurosift.app)\n",
    "    - [Example: 000728/sub-495727000/sub-495727000_ses-51254258-StimC_behavior+image+ophys.nwb](https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/0205b9b1-10c4-467c-b027-20bbbfcce3a0/download/&dandisetId=001172&dandisetVersion=0.260129.0829)\n",
    "- [OME: Neuroglancer](# TODO: Kabi give good generic link for this?) for volumetric image data.\n",
    "    - [Example: ??? TODO for Kabi]\n",
    "- [OME: NeuroGlass](https://neuroglass.io) for volumetric image data.\n",
    "    - [Example: ??? TODO for Kabi]\n",
    "\n",
    "For readers interested in exploring tools compatible with DANDI-supported data formats, refer to:\n",
    "- [NWB: Analysis Tools](https://nwb-overview.readthedocs.io/en/latest/tools/analysis_tools_home.html#analysis-and-visualization-tools)\n",
    "- [BIDS: Analysis Tools](https://bids.neuroimaging.io/tools/others.html#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362bd15",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal of this section is to demonstrate loading a portion of data from your dataset, and reveal something about its structure.\n",
    "1. Load an object from S3\n",
    "2. Show the structure of data in the object\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: Can you show us an example of downloading and loading data from your dataset?\n",
    "\n",
    "Data contents from files on the DANDI archive can either be downloaded or streamed directly from S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "id": "963d3cff",
   "metadata": {},
   "source": [
    "# Look up a specific file asset from a different Dandiset\n",
    "dandiset = client.get_dandiset(dandiset_id=\"000728\")\n",
    "dandi_filename = \"sub-491604983/sub-491604983_ses-501560436-StimC_behavior+image+ophys.nwb\"\n",
    "asset = dandiset.get_asset_by_path(path=dandi_filename)\n",
    "\n",
    "# Download entire file (alter the base directory as needed)\n",
    "output_path = Path.cwd() / Path(dandi_filename).name\n",
    "asset.download(filepath=output_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "686af22a",
   "metadata": {},
   "source": [
    "To open the file after the download completes, we can use the [PyNWB](https://pynwb.readthedocs.io/en/stable/)\n",
    "library to read the NWB file and display the basic content layout."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed25fd6e",
   "metadata": {},
   "source": [
    "nwbfile = read_nwb(path=output_path)\n",
    "print(nwbfile)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When running the notebook in compatible environments (_e.g._, Jupyter), you can also interact with the filetree.",
   "id": "a622f1ba6d3c3d43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nwbfile",
   "id": "8f3b6d22688d5370",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b9062e1",
   "metadata": {},
   "source": [
    "Specific data arrays can be accessed by traversing the NWB file structure.\n",
    "\n",
    "For example, the $\\Delta F/F$ time series derived from the raw two-photon calcium imaging can be found under the\n",
    "'processing' module."
   ]
  },
  {
   "cell_type": "code",
   "id": "d7209334",
   "metadata": {},
   "source": [
    "df_over_f_array = nwbfile.processing[\"ophys\"][\"DfOverF\"][\"DfOverF\"].data\n",
    "\n",
    "# Get a subset of the data for visualization\n",
    "# Note that the `df_over_f_array` has shape `number of frames x number of regions of interest`\n",
    "# reflecting dimensions of `time x ROIs`\n",
    "time_series_data = df_over_f_array[:1000, :5]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(time_series_data.shape[1]):\n",
    "    plt.plot(time_series_data[:, i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('ΔF/F')\n",
    "plt.title('Calcium Imaging Time Series (ΔF/F)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that all data access when reading from an NWB file is 'lazy' in the sense that data arrays are not read into memory\n",
    "until explicitly requested via slicing operations.\n",
    "\n",
    "This is particularly useful when working with large (> 60 GB) datasets that\n",
    "may not otherwise fit into memory.\n",
    "\n",
    "Additionally, some files on the DANDI archive can be quite large (up to TB-size files in multi-TB Dandisets)!\n",
    "\n",
    "Instead of downloading these, you can stream data directly from S3 using any of the [libraries supported by PyNWB](https://pynwb.readthedocs.io/en/stable/tutorials/advanced_io/streaming.html).\n",
    "\n",
    "The previous command can be adapted to stream directly from S3."
   ],
   "id": "e8260c9e1292192a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "rem_file = remfile.File(url=s3_url)\n",
    "h5py_file = h5py.File(name=rem_file, mode=\"r\")\n",
    "io = NWBHDF5IO(file=h5py_file)\n",
    "streamed_nwbfile = io.read()\n",
    "\n",
    "streamed_df_over_f_array = streamed_nwbfile.processing[\"ophys\"][\"DfOverF\"][\"DfOverF\"].data\n",
    "\n",
    "streamed_time_series_data = streamed_df_over_f_array[:1000, :5]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(streamed_time_series_data.shape[1]):\n",
    "    plt.plot(streamed_time_series_data[:, i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (frames)')\n",
    "plt.ylabel('ΔF/F')\n",
    "plt.title('Calcium Imaging Time Series (ΔF/F)')\n",
    "plt.show()"
   ],
   "id": "30c30a8f2dca6443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal here is to visualize some aspect of your dataset in order to help users understand it. In addition to helping users of your dataset understand the dataset, an additional goal is to impress!\n",
    "\n",
    "Please demonstrate any data preprocessing or reshaping required for your visualization(s).\n",
    "\n",
    "https://www.reddit.com/r/dataisbeautiful/ for inspiration.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: A picture is worth a thousand words. Show us a visual (or several!) from your dataset that either illustrates something informative about your dataset, or that you think might excite someone to dig in further.\n",
    "\n",
    "Using the same Dandiset as the previous section, we can visualize the connection between the segmentation and the\n",
    "cellular imaging by overlying the summary image with the ROIs."
   ],
   "id": "60da35825a98e0d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "summary_image = nwbfile.processing[\"ophys\"][\"SummaryImages\"][\"mean_image\"].data\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(summary_image[:], cmap='gray')\n",
    "plt.title('Mean Summary Image of Imaging Field')\n",
    "plt.xlabel('X (pixels)')\n",
    "plt.ylabel('Y (pixels)')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f3a34845447fca7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "summary_image = nwbfile.processing[\"ophys\"][\"SummaryImages\"][\"maximum_intensity_projection\"]\n",
    "summary_image"
   ],
   "id": "8375249105fd33ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\"PlaneSegmentation\"]\n",
    "\n",
    "# TODO: cody finish making pretty overlay of ROI to max proj"
   ],
   "id": "422141d366b519c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T18:48:20.860160100Z",
     "start_time": "2026-01-29T18:48:20.848519400Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "The examples above showcase optophysiology data, but DANDI hosts diverse neurophysiology modalities.\n",
    "\n",
    "Let's explore some other data types - such as electrophysiology!"
   ],
   "id": "983a4ff08179a91b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Heberto waveforms",
   "id": "7eab3bf056c8a333",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DANDI doesn't just host neural data, either - it is quite common for Dandisets to include behavioral data as well.\n",
    "\n",
    "Let's take a look at one of the most common behavioral data types - pose estimated video!"
   ],
   "id": "a5d071437d215bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Heberto single frame",
   "id": "7c413a5985293d1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "183c8b85-ed1c-4f2c-bd0e-fbfbc67c4723",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is less prescriptive / freeform than previous sections. The goal here is to show an opinionated example of answering a question using your data. The scale of your dataset may preclude a full example, and so feel free to limit the scope of this example (e.g. work on a subset of data). Users should be able to replicate your example in this notebook, and get a sense of how they would scale up.\n",
    "\n",
    "A \"toy\" example is better than no example.\n",
    "\n",
    "Ideally, your example would:\n",
    "1. Transmit some of your domain & dataset experience to the reader, drawing on your own work as much as possible\n",
    "2. Provide a jumping off point for users to extend your work, and do novel work of their own.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one question that you have answered using these data? Can you show us how you came to that answer?\n",
    "\n",
    "Focusing on the optophysiology example used above - the Visual Coding project by the Allen Institute - one question\n",
    "that was addressed involves characterizing population-level response characteristics across\n",
    "visual cortex.\n",
    "\n",
    "\n",
    "Multiple stimuli (natural scenes, drifting gratings, static gratings) were presented to each subject over the course\n",
    "of the experiment. Different structures within the visual cortex were targeted across subjects. The neural responses during each presentation were then characterized to\n",
    " show differing response properties across visual areas. This demonstrated that different cortical layers have distinct\n",
    " response properties and tuning characteristics. The experiments also quantified how correlated activity between\n",
    " neurons affects information coding by showed that noise correlations are stronger between neurons with similar tuning\n",
    " properties. Additional findings demonstrate that correlations are modulated by behavioral state (running vs.\n",
    " stationary movements).\n",
    "\n",
    "A full reproducible analysis of this work can be found through its very detailed [example notebook](https://github.com/dandi/example-notebooks/blob/master/000728/AllenInstitute/visual_coding_ophys_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf645724-3108-4ada-a832-10b3431eb8e2",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is, like the previous one, intended to be freeform / non-prescriptive. The goal here is to provide a challenge to the community to do something novel with your dataset. That can either be novel in terms of the task, or novel in terms of methodological or computational approach.\n",
    "\n",
    "Another way to consider this section, is as a wishlist. If you were less constrained by time, cost, skill, etc., what would you like to see achieved using these data? \n",
    "\n",
    "The challenge should, however, be somewhat realistic. A challenge that assumes e.g. original data collection, is likely to go unanswered.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one unanswered question that you think could be answered using these data? Do you have any recommendations or advice for someone wanting to answer this question?\n",
    "\n",
    "One such proposal might be how do different visual cortical areas (V1, LM, AL, PM) coordinate their activity over time\n",
    "during naturalistic scene viewing, and can we identify temporal \"routing\" patterns that predict behavioral state transitions?\n",
    "\n",
    "While the Visual Coding dataset(s) have characterized individual area responses, differences across cell types, and\n",
    "other correlations, the temporal dynamics of information flow between areas during natural scene processing remains\n",
    "less explored - particularly how running vs. stationary states modulate inter-area communication.\n",
    "\n",
    "It is worth mentioning in this context that the NWB group hosts a regular [NeuroDataReHack event](https://nwb.org/events/hck26-2026-janelia-ndrh/)\n",
    "where researchers are brought together to work precisely on such questions of how to analyze existing datasets in\n",
    "novel ways, rather than running entirely new experiments. Check the [NWB Events](https://nwb.org/events/) page and\n",
    "sign up for the newsletter to stay informed about these kinds of events!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "51c878722b2cc2ce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
